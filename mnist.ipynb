{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/mnist.py  \n",
    "Builds the MNIST network.  \n",
    "1. inference() - Builds the model as far as is required for running the network forward to make predictions.\n",
    "2. loss() - Adds to the inference model the layers required to generate loss.\n",
    "3. training() - Adds to the loss model the Ops required to generate and apply gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = 10\n",
    "IMAGE_SIZE = 28\n",
    "IMAGE_PIXELS = IMAGE_SIZE * IMAGE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def ｉｎference(images, hidden1_units, hidden2_units):\n",
    "    \"\"\"\n",
    "    hiden1_units: size of first hidden layer\n",
    "    \"\"\"\n",
    "    # hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(tf.truncated_normal([IMAGE_PIXELS, hidden1_units],\n",
    "                              stddev=1.0 / math.sqrt(float(IMAGE_PIXELS))))\n",
    "        biases = tf.Variable(tf.zeros([hidden1_units]))\n",
    "        hidden1 = tf.nn.relu(tf.matmul(images, weights) + biases)\n",
    "\n",
    "    #hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(tf.truncated_normal([hidden1_units, hidden2_units],\n",
    "                              stddev=1.0 / math.sqrt(float(hidden1_units))))\n",
    "        biases = tf.Variable(tf.zeros([hidden2_units]))\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "\n",
    "    # Linear\n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(tf.truncated_normal([hidden2_units, NUM_CLASSES],\n",
    "                              stddev=1.0 / math.sqrt(float(hidden2_units))))\n",
    "        biases = tf.Variable(tf.zeros([NUM_CLASSES]))\n",
    "        logits = tf.matmul(hidden2, weights) + biases\n",
    "    \n",
    "    return logits #logits.shape　[batchsize, num_classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def loss(logits, labels):\n",
    "    \"\"\"Calculates the loss from the logits and the labels.\n",
    "      Args:\n",
    "          logits: Logits tensor, float - [batch_size, NUM_CLASSES].\n",
    "          labels: Labels tensor, int32 - [batch_size].\n",
    "      Returns:\n",
    "          loss: Loss tensor of type float.\n",
    "      \"\"\"\n",
    "    labels = tf.to_int64(labels)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels)\n",
    "    #ｔｆ.nn.sparse_softmax_cross_entropy_with_logits可以自动将ｌａｂｅｌｓ转成１－ｈｏｔ编码,然后计算\n",
    "    return tf.reduce_mean(cross_entropy) # renturn loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training(loss, learning_rate):\n",
    "    \"\"\"\n",
    "    return train operation\n",
    "    \"\"\"\n",
    "    # 写ｓｕｍｍａｒｙ\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    global_step = tf.Variable(0, trainable=False) # a variable to track the global step\n",
    "    train_op = optimizer.minimize(loss, global_step = global_step) # global_step is a counter\n",
    "    return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluation(logits, labels):\n",
    "    # It returns a bool tensor with shape [batch_size]\n",
    "    correct = tf.nn.in_top_k(logits, labels, k = 1)\n",
    "    return tf.reduce_sum(tf.cast(correct, tf.int32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/mnist/fully_connected_feed.py  \n",
    "Trains and Evaluates the MNIST network using a feed dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os.path\n",
    "import sys\n",
    "import time\n",
    "\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Basic model parameters as external flags.\n",
    "FLAGS = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def placeholder_inputs(batch_size):\n",
    "    images_placeholder = tf.placeholder(tf.float32, shape=(batch_size, IMAGE_PIXELS))\n",
    "    labels_placeholder = tf.placeholder(tf.int32, shape=(batch_size))\n",
    "    return images_placeholder, labels_placeholder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fill_feed_dict(data_set, images_pl, labels_pl):\n",
    "    images_feed, labels_feed = data_set.next_batch(FLAGS.batch_size, FLAGS.fake_data)\n",
    "    feed_dict = {images_pl: images_feed, labels_pl: labels_feed}\n",
    "    return feed_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def do_eval(sess, eval_correct, image_placeholder, label_placeholder, data_set):\n",
    "    true_count = 0 # counts #correct predictions\n",
    "    steps_per_epoch = data_set.num_examples // FLAGS.batch_size\n",
    "    num_examples = steps_per_epoch * FLAGS.batch_size\n",
    "    \n",
    "    for step in xrange(steps_per_epoch):\n",
    "        feed_dict = fill_feed_dict(data_set, image_placeholder, label_placeholder)\n",
    "        true_count += sess.run(eval_correct, feed_dict = feed_dict)\n",
    "        precision = float(true_count) / num_examples\n",
    "        print('  Num examples: %d  Num correct: %d  Precision @ 1: %0.04f' %\n",
    "              (num_examples, true_count, precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    data_sets = input_data.read_data_sets(FLAGS.input_data_dir, FLAGS.fake_data)\n",
    "    \n",
    "    # tell tf that the model will be built into the default graph\n",
    "    with tf.Graph().as_default():\n",
    "        images_placeholder, labels_placeholder = placeholder_inputs(FLAGS.batch_size)\n",
    "\n",
    "        logits = inference(images_placeholder, FLAGS.hidden1, FLAGS.hidden2)\n",
    "        loss_ = loss(logits, labels_placeholder)\n",
    "        train_op = training(loss_, FLAGS.learning_rate)\n",
    "        eval_correct = evaluation(logits, labels_placeholder)\n",
    "        \n",
    "        # Build the summary Tensor based on the TF collection of Summaries.\n",
    "        summary = tf.summary.merge_all()\n",
    "        \n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        # create a saver for writing training checkpoints\n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        sess = tf.Session()\n",
    "        \n",
    "        # Instantiate a SummaryWriter to output summaries and the Graph.\n",
    "        summary_writer = tf.summary.FileWriter(FLAGS.log_dir, sess.graph)\n",
    "        \n",
    "        sess.run(init)\n",
    "        \n",
    "        for step in xrange(FLAGS.max_steps):\n",
    "            start_time = time.time()\n",
    "            feed_dict = fill_feed_dict(data_sets.train, images_placeholder, labels_placeholder)\n",
    "            \n",
    "            _, loss_value = sess.run([train_op, loss_], feed_dict = feed_dict)\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            if step % 100 == 0:\n",
    "                print('Step %d: loss = %.2f (%.3f sec)' % (step, loss_value, duration))\n",
    "                summary_str = sess.run(summary, feed_dict=feed_dict)\n",
    "                summary_writer.add_summary(summary_str, step)\n",
    "                summary_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def main(_):\n",
    "    if tf.gfile.Exists(FLAGS.log_dir):\n",
    "        tf.gfile.DeleteRecursively(FLAGS.log_dir)\n",
    "    tf.gfile.MakeDirs(FLAGS.log_dir)\n",
    "    run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/tensorflow/mnist/input_data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/tensorflow/mnist/input_data/t10k-labels-idx1-ubyte.gz\n",
      "Step 0: loss = 2.31 (0.013 sec)\n",
      "Step 100: loss = 2.14 (0.004 sec)\n",
      "Step 200: loss = 1.86 (0.003 sec)\n",
      "Step 300: loss = 1.57 (0.003 sec)\n",
      "Step 400: loss = 1.32 (0.002 sec)\n",
      "Step 500: loss = 0.84 (0.003 sec)\n",
      "Step 600: loss = 0.80 (0.003 sec)\n",
      "Step 700: loss = 0.60 (0.003 sec)\n",
      "Step 800: loss = 0.63 (0.002 sec)\n",
      "Step 900: loss = 0.57 (0.003 sec)\n",
      "Step 1000: loss = 0.48 (0.003 sec)\n",
      "Step 1100: loss = 0.56 (0.051 sec)\n",
      "Step 1200: loss = 0.43 (0.004 sec)\n",
      "Step 1300: loss = 0.37 (0.003 sec)\n",
      "Step 1400: loss = 0.35 (0.003 sec)\n",
      "Step 1500: loss = 0.45 (0.003 sec)\n",
      "Step 1600: loss = 0.37 (0.003 sec)\n",
      "Step 1700: loss = 0.45 (0.003 sec)\n",
      "Step 1800: loss = 0.36 (0.003 sec)\n",
      "Step 1900: loss = 0.38 (0.003 sec)\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cis/anaconda3/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2889: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\n",
    "        '--learning_rate',\n",
    "        type=float,\n",
    "        default=0.01,\n",
    "        help='Initial learning rate.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--max_steps',\n",
    "        type=int,\n",
    "        default=2000,\n",
    "        help='Number of steps to run trainer.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--hidden1',\n",
    "        type=int,\n",
    "        default=128,\n",
    "        help='Number of units in hidden layer 1.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--hidden2',\n",
    "        type=int,\n",
    "        default=32,\n",
    "        help='Number of units in hidden layer 2.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--batch_size',\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help='Batch size.  Must divide evenly into the dataset sizes.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--input_data_dir',\n",
    "        type=str,\n",
    "        default='/tmp/tensorflow/mnist/input_data',\n",
    "        help='Directory to put the input data.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--log_dir',\n",
    "        type=str,\n",
    "        default='/tmp/tensorflow/mnist/logs/fully_connected_feed',\n",
    "        help='Directory to put the log data.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--fake_data',\n",
    "        default=False,\n",
    "        help='If true, uses fake data for unit testing.',\n",
    "        action='store_true'\n",
    "    )\n",
    "\n",
    "    FLAGS, unparsed = parser.parse_known_args()\n",
    "    tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-364ae2e8b7f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mFLAGS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munparsed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_known_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0munparsed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/cis/anaconda3/lib/python3.5/site-packages/tensorflow/python/platform/app.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(main, argv)\u001b[0m\n\u001b[1;32m     41\u001b[0m   \u001b[0;31m# Call the main function, passing through any arguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m   \u001b[0;31m# to the final program.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m   \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mflags_passthrough\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mSystemExit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
